anon8231489123_vicuna-13b-GPTQ-4bit-128g$:
  loader: GPTQ-for-LLaMa
  cpu_memory: 0
  auto_devices: false
  disk: false
  cpu: false
  bf16: false
  load_in_8bit: false
  trust_remote_code: false
  load_in_4bit: false
  compute_dtype: float16
  quant_type: nf4
  use_double_quant: false
  wbits: '4'
  groupsize: '128'
  model_type: llama
  pre_layer: 0
  triton: false
  desc_act: false
  no_inject_fused_attention: false
  no_inject_fused_mlp: false
  no_use_cuda_fp16: false
  threads: 0
  n_batch: 512
  no_mmap: false
  mlock: false
  n_gpu_layers: 0
  n_ctx: 2048
  llama_cpp_seed: 0.0
  gpu_split: ''
  max_seq_len: 2048
  compress_pos_emb: 1
  gpu_memory_0: 0
TehVenom_Pygmalion-7b-4bit-GPTQ-Safetensors$:
  loader: GPTQ-for-LLaMa
  cpu_memory: 0
  auto_devices: false
  disk: false
  cpu: false
  bf16: false
  load_in_8bit: false
  trust_remote_code: false
  load_in_4bit: false
  compute_dtype: float16
  quant_type: nf4
  use_double_quant: false
  wbits: '4'
  groupsize: None
  model_type: llama
  pre_layer: 0
  triton: false
  desc_act: false
  no_inject_fused_attention: false
  no_inject_fused_mlp: false
  no_use_cuda_fp16: false
  threads: 0
  n_batch: 512
  no_mmap: false
  mlock: false
  n_gpu_layers: 0
  n_ctx: 2048
  llama_cpp_seed: 0.0
  gpu_split: ''
  max_seq_len: 2048
  compress_pos_emb: 1
  gpu_memory_0: 0
TheBloke_vicuna-AlekseyKorshuk-7B-GPTQ-4bit-128g$:
  loader: GPTQ-for-LLaMa
  cpu_memory: 0
  auto_devices: false
  disk: false
  cpu: false
  bf16: false
  load_in_8bit: false
  trust_remote_code: false
  load_in_4bit: false
  compute_dtype: float16
  quant_type: nf4
  use_double_quant: false
  wbits: '4'
  groupsize: '128'
  model_type: llama
  pre_layer: 0
  triton: false
  desc_act: false
  no_inject_fused_attention: false
  no_inject_fused_mlp: false
  no_use_cuda_fp16: false
  threads: 0
  n_batch: 512
  no_mmap: false
  mlock: false
  n_gpu_layers: 0
  n_ctx: 2048
  llama_cpp_seed: 0.0
  gpu_split: ''
  max_seq_len: 2048
  compress_pos_emb: 1
  gpu_memory_0: 0
